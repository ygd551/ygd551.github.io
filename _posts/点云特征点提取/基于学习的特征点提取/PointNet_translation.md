# PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation

## 摘要

点云是一种重要的几何数据结构。由于其不规则的格式，大多数研究者通常将此类数据转换为规则的三维体素网格或图像集合。然而，这种转换会导致数据体积过大，并引入额外的问题。本文设计了一种新型神经网络，可以直接处理点云数据，并充分考虑输入点集的排列不变性。我们将该网络命名为 **PointNet**，它为包括目标分类、部件分割以及场景语义解析在内的多种任务提供了统一的架构。尽管结构简单，PointNet 却具有极高的效率与优越的性能。从实验结果看，它的表现与当前最先进的方法相当，甚至更优。理论上，我们还从分析的角度阐释了网络学到了什么，以及为何该网络在输入扰动与损坏的情况下仍能保持鲁棒性。

***

## 1. 引言

本文探索了能够对三维几何数据（如点云或网格）进行推理的深度学习架构。典型的卷积网络要求输入数据具有高度规则的格式，例如图像网格或三维体素，以便实现权值共享和卷积核优化。由于点云或网格数据并不规则，大多数研究者会先将此类数据转换为规则的三维体素网格或多视图图像集合，然后再输入深度网络。然而，这种数据表示转换不仅使得数据体积大幅增加，还会引入量化误差，从而掩盖数据本身的自然不变性。

因此，本文选择直接使用点云作为三维几何的输入表示，并将相应的深度网络命名为 **PointNet**。点云是一种简单且统一的数据结构，能够避免网格数据中的组合不规则性与复杂性，因此更易于学习。然而，PointNet 的设计仍需注意的是，点云本质上只是一个点的集合，因此其成员之间的排列不应影响整体结果，这要求网络计算中必须具备一定的**对称化机制**。此外，还需考虑输入数据对刚体变换的不变性。

我们的 **PointNet** 是一种统一的网络架构，它可以直接以点云为输入，并输出整个输入的类别标签或每个点的分割/部件标签。该网络的基本结构出乎意料地简单：在初始阶段，每个点被**独立且相同地**处理。在最基本的形式下，每个点仅由其三个坐标 $(x, y, z)$ 表示；若需要，还可通过计算法向量或其他局部/全局特征来扩展其维度。

我们方法的关键在于使用一个**对称函数——最大池化（max pooling）**。实际上，网络学习了一组优化函数或判别准则，用于选择点云中最具代表性或最有信息的点，并编码其被选中的原因。网络的最终全连接层将这些学习到的最优值聚合成一个全局描述符，用于整个形状的分类；或用于预测每个点的标签（形状分割）。

由于我们的输入格式可以方便地进行**刚体或仿射变换**（每个点独立变换），因此我们在 PointNet 之前加入了一个**空间变换网络（spatial transformer network）**，用于尝试将输入数据标准化，从而进一步提升性能。

我们同时进行了**理论分析与实验评估**。结果表明，我们的网络能够逼近任意连续的集合函数。更有趣的是，网络学会通过少量关键点来概括输入点云，这些关键点在可视化上大致对应于物体的“骨架”。理论分析还解释了为什么 PointNet 对输入点的微小扰动具有高度鲁棒性，并能抵抗点的插入（离群点）或删除（缺失数据）。

在多个基准数据集上（包括形状分类、部件分割与场景分割），我们将 PointNet 与基于多视图与体素表示的最先进方法进行了对比。结果显示，在统一的架构下，**PointNet 不仅运行速度更快，而且性能与现有最佳方法相当甚至更优**。

***

### 我们工作的主要贡献如下：

* 我们设计了一种新颖的深度网络架构，适用于处理无序的三维点集；
* 我们展示了该网络如何被训练以执行三维形状分类、形状部件分割以及场景语义解析任务；
* 我们对方法的稳定性与效率进行了系统的**理论与实证分析**；
* 我们可视化了网络中部分神经元所学习到的三维特征，并给出了直观的性能解释。

无序集合的神经网络处理问题是一个非常**普遍且基础的课题**——我们相信本文提出的思想同样可以迁移到其他领域。

***

## 2. 相关工作

### 点云特征（Point Cloud Features）

大多数现有的点云特征都是针对特定任务**手工设计**的。点特征通常编码点的某些统计属性，并被设计为对某些变换保持不变性，这些不变性一般分为**内在不变性**（intrinsic）和**外在不变性**（extrinsic）两类。特征还可以划分为**局部特征**与**全局特征**。然而，对于特定任务而言，找到最优的特征组合并非易事。

### 基于三维数据的深度学习（Deep Learning on 3D Data）

三维数据有多种常见的表示形式，因此出现了多种学习方法：

* **体素卷积网络（Volumetric CNNs）**：如 \[28, 17, 18] 是最早将三维卷积神经网络应用于体素化形状的研究。然而，体素表示受限于数据稀疏性和三维卷积的计算成本，难以处理高分辨率数据。FPNN \[1] 和 Vote3D \[2] 提出了专门应对稀疏性的问题方法，但它们仍在稀疏体素上操作，因此在处理超大点云时依然具有挑战。

* **多视图卷积网络（Multiview CNNs）**：如 \[23,18] 将三维点云或形状渲染为二维图像，再使用二维卷积网络进行分类。借助成熟的图像 CNN 结构，这类方法在形状分类与检索任务上取得了主导性表现 \[21]。但要将其扩展到场景理解或点级任务（如点分类、形状补全）并非易事。

* **谱卷积网络（Spectral CNNs）**：一些最新工作 \[4,6] 将谱卷积应用于网格结构。然而这些方法通常受限于流形网格（如有机体），难以推广到非等距形状（如家具）。

* **基于特征的深度神经网络（Feature-based DNNs）**：如 \[6,8] 首先提取传统形状特征并将其转换为向量，然后使用全连接网络进行分类。这类方法受限于手工特征的表达能力。

### 基于无序集合的深度学习（Deep Learning on Unordered Sets）

从数据结构的角度看，点云是一个**无序的向量集合**。尽管深度学习的大多数研究集中于规则输入（如序列、图像、体素等），但在处理无序点集的神经网络研究中，工作仍然较少。

Oriol Vinyals 等人 \[25] 的一项最新工作探讨了该问题。他们使用带注意力机制的\*\*读-处理-写网络（read-process-write network）\*\*来处理无序输入集合，并展示了其在数字排序任务中的能力。然而，他们的研究主要针对通用集合与自然语言处理任务，**缺乏几何信息的考虑**。



## 3. 问题定义（Problem Statement）

我们设计了一个能够**直接处理无序点集输入**的深度学习框架。一个点云可表示为一组三维点：\
\[\
{ P\_i \mid i = 1, \ldots, n }\
]\
其中，每个点 $P\_i$ 是一个向量，包含其三维坐标 $(x, y, z)$，**以及可能的附加特征通道（如颜色、法向量等）**。为了简化叙述并保持清晰性，除非特别说明，我们在本文中仅使用点的三维坐标 $(x, y, z)$ 作为输入通道。

对于**目标分类任务（object classification）**，输入点云可以直接从三维形状中采样，或从场景点云中预先分割得到。我们提出的深度网络将输出 $k$ 个得分，对应于所有 $k$ 个候选类别。

对于**语义分割任务（semantic segmentation）**，输入既可以是单个物体（用于部件区域分割），也可以是三维场景中的子体素块（用于物体区域分割）。在这种情况下，我们的模型将输出一个大小为 $n \times m$ 的分数矩阵，其中 $n$ 为输入点的数量，$m$ 为语义子类别的数量，即为每个点预测属于每个语义类别的概率或得分。



## 4. 基于点集的深度学习（Deep Learning on Point Sets）

我们的网络架构（见第 4.2 节）受到了 $\mathbb{R}^n$ 空间中点集特性的启发（见第 4.1 节）。

***

### 4.1 点集在 $\mathbb{R}^n$ 空间中的特性

我们的输入是欧几里得空间中的点集子集，其具有以下三个主要特性：

* **无序性（Unordered）**\
  与图像中的像素数组或体素网格中的体素数组不同，点云是一组**无特定顺序的点集合**。换句话说，一个接收 $N$ 个三维点作为输入的网络，必须对输入点的 $N!$ 种排列保持不变性。

* **点之间的相互作用（Interaction among points）**\
  点集位于带有距离度量的空间中，因此各点并非孤立存在。相邻的点构成有意义的局部结构。因此，模型需要具备捕获**邻近点之间局部结构**以及**局部结构间组合关系**的能力。

* **变换不变性（Invariance under transformations）**\
  作为几何对象，点集的表示应对某些变换保持不变。例如，对点云整体进行旋转或平移，不应改变其全局类别标签或分割结果。

***

### 4.2 PointNet 架构

完整的网络结构如图 2 所示，其中分类网络与分割网络共享了大部分结构。请参考图 2 的注释了解整体流程。

我们的网络包含三个关键模块：

1. **对称函数层（max pooling）** —— 用于从所有点中聚合信息；
2. **局部与全局信息融合结构**；
3. **双重对齐网络（joint alignment networks）** —— 分别对输入点与点特征进行对齐。

下面将分别阐述这些设计背后的原因。

***

#### （1）无序输入的对称函数（Symmetry Function for Unordered Input）

为了让模型对输入点的排列顺序不敏感，常见有三种策略：

1. **对输入进行排序**，使其符合某种规范顺序；
2. **将输入视为序列**，并使用 RNN 进行训练，同时通过不同排列进行数据增强；
3. **使用对称函数（symmetric function）** 聚合来自各点的信息。

对称函数接受 $n$ 个向量作为输入，输出一个对输入顺序不敏感的新向量。常见的加法（+）与乘法（×）都是对称函数的例子。

（1）排序：尽管排序听起来简单，但在高维空间中，并不存在对点扰动稳定的排序方式。如果这种排序存在，它将定义从高维空间到一维实数轴的双射映射。而要求这种映射在降维时保持空间邻近性是不可能的。因此，排序不能完全解决输入无序问题，网络也难以学习一致的输入输出映射。实验（图 5）表明，直接在排序后的点集上使用 MLP 效果较差，仅略优于在无序输入上直接训练。

（2）RNN：RNN 方法将点集视为序列，并希望通过随机排列序列训练使模型对顺序不敏感。但在 “OrderMatters” 一文中，作者证明了输入顺序确实会影响结果。RNN 在输入长度较短（几十个元素）时具有一定鲁棒性，但对于成千上万个点的点集而言，难以扩展。我们的实验同样验证了基于 RNN 的模型性能低于所提方法（见图 5）。

（3）使用对称函数：我们提出的方法是：通过对集合中各元素施加变换并应用对称函数来近似集合函数：

$$\
f\left({x\_1, \ldots, x\_n}\right) \approx g\left(h(x\_1), \ldots, h(x\_n)\right),\
$$

其中，$f: 2^{\mathbb{R}^N} \rightarrow \mathbb{R}$，$h: \mathbb{R}^N \rightarrow \mathbb{R}^K$，$g: \underbrace{\mathbb{R}^K \times \cdots \times \mathbb{R}^K}\_n \rightarrow \mathbb{R}$ 为一个对称函数。

在实践中，我们使用\*\*多层感知机（MLP）**近似 $h$，并用一个单变量函数与**最大池化（max pooling）\*\*组合近似 $g$。实验表明这一简单结构表现出色。通过不同的 $h$，我们可以学习多个 $f$ 来捕获集合的不同属性。

虽然该模块结构简单，但其具有丰富的数学性质（见第 5.3 节）并在多种任务中实现了优异表现（见第 5.1 节）。正因其简洁性，我们能够在第 4.3 节中给出理论分析。

***

#### （2）局部与全局信息聚合（Local and Global Information Aggregation）

前一节输出的向量为 $\left[f_1, \ldots, f_K\right]$，即输入点集的**全局特征描述符**。\
对于分类任务，我们可以在该全局特征上训练 SVM 或 MLP 分类器。\
但对于分割任务，需要同时结合**局部几何信息**与**全局语义信息**。

解决方案如图 2（Segmentation Network）所示：\
在计算出全局特征向量后，我们将其**拼接（concatenate）到每个点的局部特征上，使每个点的特征同时包含局部与全局信息。\
随后，我们基于这些组合特征提取新的点级特征。\
通过这种方式，网络可以预测依赖于局部几何与全局语义的点级属性（如法向量或语义标签）。\
实验结果表明，该方法在形状部件分割与场景分割任务中均达到了**当前最优性能。

***

#### （3）联合对齐网络（Joint Alignment Network）

点云的语义标注应当在几何变换（如刚体变换）下保持不变。因此，我们希望网络学习到的表示同样对这些变换不敏感。

一个自然的做法是：在特征提取前，将输入点集**对齐到标准坐标空间**。\
Jaderberg 等人提出的 **Spatial Transformer Network (STN)** 能通过采样与插值实现二维图像的对齐。然而，点云输入的形式使我们能够更简单地实现同样的目标。

我们通过一个小型子网络（称为 **T-Net**，见图 2）预测一个**仿射变换矩阵**，并直接将其应用于输入点坐标。\
该子网络结构与主网络类似，由点独立特征提取、最大池化及全连接层组成。\
（更多 T-Net 细节见补充材料。）

这一思想还可进一步扩展至**特征空间的对齐**。\
我们可以在点特征上再加入一个对齐网络，预测特征变换矩阵，用以对齐来自不同输入点云的特征表示。\
然而，特征空间的变换矩阵维度远高于空间变换矩阵，这使优化更加困难。\
因此，我们在 softmax 训练损失中加入一个正则化项，以约束特征变换矩阵接近正交矩阵：

$$\
L\_{reg} = |I - A A^T|\_F^2,\
$$

其中，$A$ 为由小型网络预测的特征对齐矩阵。\
正交变换不会丢失输入信息，因此是理想的对齐形式。\
我们发现，加入该正则化项后，优化过程更加稳定，模型性能也得到提升。



## 4.3 理论分析（Theoretical Analysis）

### 泛函逼近能力（Universal Approximation）

首先，我们展示了神经网络对连续集合函数的**泛函逼近能力**。\
根据集合函数的连续性，直观上，对输入点集的小扰动不应显著改变函数值，例如分类或分割得分。

形式化地，设

$$
\mathcal{X}=\left\{S: S \subseteq[0,1]^m \text { and }|S|=n\right\}, 
$$

$f:\mathcal{X} \rightarrow \mathbb{R}$是定义在 $\mathcal{X}$ 上的**连续集合函数**，其连续性是基于 Hausdorff 距离 $d\_H(\cdot, \cdot)$，即对于任意 $\epsilon > 0$，存在 $\delta > 0$，使得对于任意 $S, S' \in \mathcal{X}$，如果 $d\_H(S, S') < \delta$，则 $\left| f(S) - f(S') \right| < \epsilon$。

我们的定理说明，当最大池化层的神经元足够多（即公式 (1) 中的 $K$ 足够大）时，$f$ 可以被我们的网络任意逼近。

***

#### **定理 1**

设 $f: \mathcal{X} \rightarrow \mathbb{R}$ 是关于 Hausdorff 距离 $d\_H(\cdot, \cdot)$ 的连续集合函数。\
则对于任意 $\epsilon > 0$，存在一个连续函数 $h$ 和一个对称函数

$$\
g(x_1, \ldots, x_n) = \gamma \circ \operatorname{MAX},\
$$

使得对于任意 $S \in \mathcal{X}$，

$$\
\left| f(S) - \gamma \left( \underset{x_i \in S}{\operatorname{MAX}} { h(x_i) } \right) \right| < \epsilon\
$$

其中 $x_1, \ldots, x_n$ 是 $S$ 中的所有元素（顺序任意），\
$\gamma$ 是连续函数，**MAX** 是向量最大值操作符，它接受 $n$ 个向量作为输入，并返回每个分量的最大值组成的新向量。

***

该定理的证明见补充材料。\
关键思想是，在最坏情况下，网络可以通过将空间划分为等大小的体素，把点云转换为**体素表示**。\
然而在实际中，网络会学习一种更智能的策略来探测空间，这将在点函数可视化中体现。

***

### 瓶颈维度与稳定性（Bottleneck Dimension and Stability）

理论和实验表明，网络的**表达能力**受最大池化层维度的强烈影响（即公式 (1) 中的 $K$）。\
下面给出一个分析，同时揭示了模型稳定性相关的性质。

定义

$$\
\mathbf{u} = \underset{x_i \in S}{\operatorname{MAX}} { h(x_i) }\
$$

为 $f$ 的子网络，它将 $[0,1]^m$ 中的点集映射为 $K$ 维向量。\
下述定理说明输入中存在的小扰动或额外噪声点**通常不会改变网络输出**。

***

#### **定理 2**

设 $\mathbf{u}: \mathcal{X} \rightarrow \mathbb{R}^K$，满足

$$\
\mathbf{u} = \underset{x_i \in S}{\operatorname{MAX}} { h(x_i) }, \quad f = \gamma \circ \mathbf{u}.\
$$

则对于 $x_i \in S$：

(a) 对任意 $S$，存在 $\mathcal{C}_S, \mathcal{N}_S \subseteq \mathcal{X}$，\
使得当 $\mathcal{C}_S \subseteq T \subseteq \mathcal{N}_S$ 时，有 $f(T) = f(S)$。

(b) $\left| \mathcal{C}\_S \right| \leq K$

***

#### **定理含义**

* (a) 表明，如果保留集合 $\mathcal{C}_S$ 中的所有点，$f(S)$ 对输入的扰动保持不变；\
  同时，加入额外噪声点（在 $\mathcal{N}_S$ 中）也不会改变结果。

* (b) 表明 $\mathcal{C}_S$ 中的点数是有限的，由 $K$ 决定。\
  换句话说，$f(S)$ **实际上完全由 $\mathcal{C}_S \subseteq S$ 中至多 $K$ 个关键点决定**。\
  我们称 $\mathcal{C}_S$ 为 $S$ 的**关键点集（critical point set）**，$K$ 为函数 $f$ 的**瓶颈维度（bottleneck dimension）**。

***

结合 $h$ 的连续性，这解释了网络对点扰动、点集损坏以及额外噪声点的**鲁棒性**。\
这种鲁棒性类似于机器学习模型中的**稀疏性原则**。

直观上，网络学会通过**稀疏关键点集**来概括一个形状。\
实验中，我们观察到关键点形成了物体的**骨架结构**。
